<br>
<p align="center">
<h1 align="center"><strong>Paper list for World Model</strong></h1>
</p>

<p align="center">
<img src="./fig/World_Model_Illustration.png" width="250">
</p>


#### We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to **yiranqin@link.cuhk.edu.cn**. Thanks for your cooperation! We also welcome your pull requests for this project!
 

<p align="center">
<img src="./fig/motivation.png" width="800">
</p>  

## ğŸ  About

Before taking action, humans make predictions based on their objectives and observations of the current environment. These predictions manifest in various forms, \eg, textual planning, visual imagination of future scene changes, or even subconscious planning at the action level. Each of these predictive capabilities is critical to the successful completion of tasks. With the development of generative models, agents driven by these models are exhibiting predictive capabilities that enable them to complete embodied tasks by making human-like predictions, high-level planning, image-based guidance, or future video prediction to drive actions. We refer to these models as World Models. Recently, these models have been widely applied across various domains spanning from developing agents to solve inference tasks to leveraging predictions for driving robots to perform specific actions.

## :collision: Update Log 

* [2024.12.10] We release the first version of the paper list for Embodied AI. This page is continually updating!



## <a id="table-of-contents">ğŸ“š Table of Contents </a>

- [Books & Surveys](#books-surveys)
- [Foundation Model](#Foundation-Model)
- [General World Model](#General-World-Model)
- [Autonomous Driving](#Autonomous-Driving)
- [Robot Manipulation](#Robot-Manipulation)
- [Indoor Navigation](#Indoor-Navigation)
- [World Model Safty](#World-Model-Safty)
- [Social Simulation](#Social-Simulation)
- [Multi-Agent World Model](#Multi-Agent)

## <a id="books-surveys"> Books & Surveys <a href="#table-of-contents">ğŸ”</a> </a> 

* **Multimodal Large Models: The New Paradigm of Artificial General Intelligence**, Publishing House of Electronics Industry (PHE), 2024       
Yang Liu, Liang Lin             
[[Page](https://hcplab-sysu.github.io/Book-of-MLM/)]      


## <a id="Foundation-Model"> Foundation Model <a href="#table-of-contents">ğŸ”</a> </a>
### Text Generation
### Image Generation
### Video Generation

## <a id="General-World-Model"> General World Model <a href="#table-of-contents">ğŸ”</a> </a> 

## <a id="Autonomous-Driving"> Autonomous Driving <a href="#table-of-contents">ğŸ”</a> </a> 


## <a id="Robot-Manipulation"> Robot Manipulation <a href="#table-of-contents">ğŸ”</a> </a> 

## <a id="Indoor-Navigation"> Indoor Navigation <a href="#table-of-contents">ğŸ”</a> </a> 

## <a id="World-Model-Safty"> World Model Safty <a href="#table-of-contents">ğŸ”</a> </a> 
### Safety Evaluation

### Safety Benchmark
SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model[paper](https://arxiv.org/abs/2406.12030)


SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/a48ad12d588c597f4725a8b84af647b5-Paper-Datasets_and_Benchmarks.pdf)


### Attack
From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking[paper](https://arxiv.org/abs/2406.14859)
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast[paper](https://arxiv.org/abs/2402.08567)

### Safety enhancement
MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance[paper](https://arxiv.org/abs/2401.02906)

## <a id="Social-Simulation"> Social Simulation <a href="#table-of-contents">ğŸ”</a> </a> 

## <a id="Multi-Agent"> Multi-Agent World Model <a href="#table-of-contents">ğŸ”</a> </a> 



   

 


## ğŸ‘ Acknowledgements

